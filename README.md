# mnist-smoothie
A warm little project that teaches a model to read handwritten digits using softmax regression. Just pixels, math, and magic âœ¨
<br>
<br>
# ğŸ§  MNIST Softmax Classifier

Welcome to the **Softmax Regression Wonderland**, where a matrix of weights tries its best to understand handwritten digits (0â€“9). It's just some pixels, a bit of math, and a softmax function doing all the heavy lifting.

<br>

## ğŸª„ What does this project do?

This tiny but mighty project trains a **Softmax Regression model** on the **MNIST dataset** using pure NumPy â€” no TensorFlow, PyTorch, or any high-level shortcuts. The model learns to classify digits based on 28Ã—28 grayscale images.

Yes, our little matrix learns to tell a 3 from an 8. ğŸ’ª

<br>

## ğŸ“¦ What's inside?

- ğŸ”¢ **One-hot encoding** for labels (because digits deserve identity too)
- ğŸ§¼ **Pixel normalization** (0â€“255 â†’ 0â€“1 range)
- ğŸ” A **251-iteration loop** to update weights (it tried hard, okay?)
- ğŸ“ˆ Manual **softmax math** and **gradient computation**
- ğŸ§® Accuracy calculated like a proud math nerd
- ğŸ’¾ Model saving via `np.savetxt` (who needs `.h5` anyway?)

<br>

## ğŸ§ª Accuracy?

On the training data:
> ğŸ† **~87.4%** accuracy â€” no deep learning, just deep thinking.

ğŸ’¡ Bonus: It even skips predictions if the softmax is too unsure (confidence matters!).

<br>

<br>

## ğŸ“ Pretrained Weights (Optional)

Already trained and tired? ğŸ¥± No worries.

You can skip training and use the **pretrained weights and bias** included in this repo:

- `weights.csv`
- `bias.csv`

Just load them like this:

```python
import numpy as np

weights = np.loadtxt("weights.csv", delimiter=",")
bias = np.loadtxt("bias.csv", delimiter=",").reshape(1, 10)


