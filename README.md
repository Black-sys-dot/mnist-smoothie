# ğŸº MNIST Smoothie  

![Python](https://img.shields.io/badge/Python-3.10-blue)
![NumPy](https://img.shields.io/badge/NumPy-Scientific%20Computing-orange)
![MNIST](https://img.shields.io/badge/Dataset-MNIST-lightgrey)
![Model](https://img.shields.io/badge/Model-Softmax%20Regression-green)
![Status](https://img.shields.io/badge/Status-Trained%20~87%25-success)

---

## ğŸ§  MNIST Softmax Classifier

Welcome to the **Softmax Regression Wonderland**,  
where a humble matrix of weights tries its best to understand handwritten digits (0â€“9).  
Itâ€™s just some pixels, a bit of math, and a softmax function doing all the heavy lifting. âœ¨  

---

## ğŸª„ What Does This Project Do?

This tiny but mighty project trains a **Softmax Regression model** on the **MNIST dataset** using pure **NumPy** â€”  
no TensorFlow, no PyTorch, no excuses.  

The model learns to classify digits from **28Ã—28 grayscale images**,  
and honestlyâ€¦ it does a pretty solid job for something built from scratch. ğŸ’ª  

---

## ğŸ“¦ Whatâ€™s Inside?

- ğŸ”¢ **One-hot encoding** for labels (because digits deserve individuality too)  
- ğŸ§¼ **Pixel normalization** â†’ 0â€“255 squished into 0â€“1  
- ğŸ” **251 training loops** â€” itâ€™s trying its best, okay?  
- ğŸ“ˆ **Manual softmax** and gradient calculations (math gang rise up ğŸ§®)  
- ğŸ’¾ Model saving via `np.savetxt` (who needs `.h5` anyway?)  

---

## ğŸ§ª Accuracy?

On the training data:  
> ğŸ† **~87.4% accuracy** â€” no deep learning, just deep thinking.  

ğŸ’¡ Bonus feature:  
If the modelâ€™s softmax confidence is too low, it *skips* predictions.  
Because even AI knows when to say, â€œI donâ€™t know, bro.â€ ğŸ¤·â€â™‚ï¸  

---

## ğŸ“ Pretrained Weights (Optional)

Already trained and tired? ğŸ¥±  
No worries â€” pretrained weights are here to save your neurons.  

Youâ€™ll find:  
- `weights.csv`  
- `bias.csv`  

Load them like this:
```python
import numpy as np

weights = np.loadtxt("weights.csv", delimiter=",")
bias = np.loadtxt("bias.csv", delimiter=",").reshape(1, 10)
